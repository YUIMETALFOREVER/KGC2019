{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KGC2019_NBC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YUIMETALFOREVER/KGC2019/blob/master/KGC2019_NBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDrkm8IXJzQ4",
        "colab_type": "text"
      },
      "source": [
        "# **Naive Bays Classifierのよる犯人推測**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mje-79dKJ8iM",
        "colab_type": "text"
      },
      "source": [
        "**セリフデータの読み込み**\n",
        "\n",
        "　犯人のセリフ　＝＝＞　cri\n",
        "\n",
        "　犯人以外のセリフ＝＞　inn\n",
        "\n",
        " 事前作業：\n",
        " 犯人のセリフデータ'talk_cri.txt'と犯人以外のセリフデータ'talk_inn.txt'をGithubからコピーしてColaboratoryにアップロードしてください。\n",
        "\n",
        " 現時点では「悪魔の足」のセリフデータのみであるため、学習には大幅に不足しており、pythonコードの動作確認にしかなりません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CLtCXh_KH9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "critalk_file = 'talk_cri.txt'  #セリフデータの指定\n",
        "inntalk_file = 'talk_inn.txt'\n",
        "\n",
        "with open(critalk_file,'r') as f:\n",
        "  critalks = f.readlines()\n",
        "with open(inntalk_file,'r') as f:\n",
        "  inntalks = f.readlines()\n",
        "critalks = [line.lower().strip(\"\\n\") for line in critalks] #改行の除去\n",
        "critalks = [line.replace(\".\",\" .\").replace(\"?\",\" ?\").replace(\",\",\" ,\").replace(\"!\",\" !\") for line in critalks]\n",
        "inntalks = [line.lower().strip(\"\\n\") for line in inntalks]\n",
        "inntalks = [line.replace(\".\",\" .\").replace(\"?\",\" ?\").replace(\",\",\" ,\").replace(\"!\",\" !\") for line in inntalks]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAP8J0TNMP0w",
        "colab_type": "text"
      },
      "source": [
        "**trainingデータとtestデータに分離**\n",
        "\n",
        "現時点ではセリフデータが極端に少ないため、テストデータは10件とする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPNX4YJvK87b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SplitIndex = 10\n",
        "testCriTalks = critalks[SplitIndex+1:]\n",
        "testInnTalks = inntalks[SplitIndex+1:]\n",
        "trainingCriTalks = critalks[:SplitIndex]\n",
        "trainingInnTalks = inntalks[:SplitIndex]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdXY0tvwMfGr",
        "colab_type": "text"
      },
      "source": [
        "**getVocabulary関数**\n",
        "\n",
        "　セリフデータを単語Listに分離\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFi39ZixLoA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getVocabulary():\n",
        "  critalkWordList = [word for line in trainingCriTalks for word in line.split()]\n",
        "  inntalkWordList = [word for line in trainingInnTalks for word in line.split()]\n",
        "  allWordList = [item for list in [critalkWordList,inntalkWordList] for item in list]\n",
        "  allWordSet = list(set(allWordList))\n",
        "  vocabulary = allWordSet\n",
        "  return vocabulary\n",
        "\n",
        "vocabulary = getVocabulary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMm8QUnBkt7l",
        "colab_type": "text"
      },
      "source": [
        "**trainingデータの生成**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-VMP-EPLmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTrainingData(cri='dummy', inn='dummy'):\n",
        "  criTaggedTrainingTalkList = [{'talk':oneTalk.split(),'label':'cri'} for oneTalk in cri]\n",
        "  innTaggedTrainingTalkList = [{'talk':oneTalk.split(),'label':'inn'} for oneTalk in inn]\n",
        "  fullTaggedTrainingData = [item for list in [criTaggedTrainingTalkList,innTaggedTrainingTalkList] for item in list]\n",
        "  trainingData = [(talk['talk'],talk['label']) for talk in fullTaggedTrainingData]\n",
        "  return trainingData\n",
        "trainingData = getTrainingData(cri=trainingCriTalks,inn=trainingInnTalks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu2u-o8U-wuA",
        "colab_type": "text"
      },
      "source": [
        "**特徴抽出**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0_1u3jA-vm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(talk):\n",
        "  talk_words = set(talk)\n",
        "  features = {}\n",
        "  for word in vocabulary:\n",
        "    features[word] = (word in talk_words)\n",
        "  return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cdc35555-66eb-47ea-d5a7-fcaed02ba5a1",
        "id": "ox0ZooucZyx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#def getTrainingNaiveBayesClassifier(extract_features, trainingData):\n",
        "#  trainingFeatures = nltk.classify.apply_features(extract_features, trainingData)\n",
        "#  trainedNBClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
        "#  return trainedNBClassifier\n",
        "\n",
        "def naiveBayesCriminalCalculator(talk):\n",
        "  talkwork = talk.lower().strip(\"\\n\")\n",
        "  talkwork = talkwork.replace(\".\",\" .\").replace(\"?\",\" ?\").replace(\",\",\" ,\").replace(\"!\",\" !\")\n",
        "  problemInstance = talkwork.split()\n",
        "  problemFeatures = extract_features(problemInstance)\n",
        "  return trainedNBClassifier.classify(problemFeatures)\n",
        "\n",
        "trainedNBClassifier = getTrainingNaiveBayesClassifier(extract_features, trainingData)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getTrain 01\n",
            "getTrain 02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvhRr-zDbo3K",
        "colab_type": "code",
        "outputId": "390b5d85-67cc-4b15-f2cb-32f9855c8932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "naiveBayesCriminalCalculator(\"You coward!.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'inn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvu_b8KMmY99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}